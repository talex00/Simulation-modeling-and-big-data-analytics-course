{"cells":[{"cell_type":"markdown","metadata":{"id":"Fuf9p9hUtL7y"},"source":["# Лабораторная работа № 7. Ансамблевые модели для прогнозирования температуры блока усиления мощности."]},{"cell_type":"markdown","metadata":{"id":"1JO0Pz5gtL73"},"source":["В работе проводится обзор основных ансамблевых моделей машинного обучения по прогнозированию временного ряда. В качестве примера рассмотрена температура блока усиления мощности."]},{"cell_type":"markdown","metadata":{"id":"qU_8SNLntL75"},"source":["## Введение\n","\n","Современные радиолокационные станции (РЛС) – это структурно-сложные радиотехнические и информационные системы, характеризующиеся высокой надежностью функционирования и большим числом цифровых компонентов в своем составе. Одним из таких компонентов является блок усиления мощности (БУМ), задача которого усиливать передаваемый или принимаемый сигнал.\n","\n","Функционирование БУМ приводит к их нагреву, что может сказаться на снижении их работоспособности или даже привести к отказу.  Техническое состояние БУМ напрямую зависит от их температуры: при достижении определенного порога блок перестает работать и начинает охлаждаться. После охлаждения до определенной температуры он снова переходит в рабоспособное состояние.\n","\n","Основная задача, решаемая в рамках данной лабораторной работы - спрогнозировать будущее значение температуры блоков усиления мощности на основании истории их функционирования и режима работы блоков. Последний определяет тип сигнала, передаваемый на БУМ, а поэтому определяет интенсивность нагрева."]},{"cell_type":"markdown","metadata":{"id":"hL6U2xPTtL77"},"source":["Существует 2 основных подхода к прогнозированию временного ряда методами машинного обучения. В обоих подходах обучающая выборка состоит из ретроспективных данных о функционировании блока. Отличие состоит в том, что выбирается в качестве целевой переменной. \n","\n","В первом подходе определяется значение временного ряда (температуры) на следующем отсчете времени (целевая переменная - 1-мерная). Тогда прогнозные значения на каком-то интервале времени будут получаться путем последовательного прогнозирования значения ряда на 1 временной отсчет. При этом ошибка неизбежно накапливается, поскольку каждое прогнозное значение имеет свою неустранимую ошибку.\n","\n","В другом подходе в качестве целевой переменной выбирается сразу интервал значений длиной *n* (целевая переменная *n*-мерная). Такой подход лишен недостатка в виде накопления ошибки, как в первом подходе. Однако, в этом случае сам метод прогнозирования сложнее и требует больших временных затрат и больших вычислительных мощностей.\n","\n","В данной лабораторной работе мы остановимся на первом подходе."]},{"cell_type":"markdown","metadata":{"id":"PERtfu7PtL79"},"source":["## Описание файла с данными"]},{"cell_type":"markdown","metadata":{"id":"pnOWaO6RtL7-"},"source":["Подключим стандартные пакеты для работы с данными и построения графиков"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EI3Aiv9BtL7_"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"Jn9HKY3RtL8C"},"source":["Загрузим файл с данными и выведем на экран первые 5 строк. Получим информацию по каждой колонке."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"0XcaM9cutL8D"},"outputs":[],"source":["df = pd.read_csv(\"Lab7_data.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wDqP7KYOtL8E"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"lMg8Eo0NtL8F"},"source":["Первая колонка **mode** обозначает номер режима работы блоков. Этот режим определяет интенсивность нагрева блоков. Анализ каждого из режимов был выполнен в предыдущей лабораторной работе. Следующие 3 колонки в датасете содержат в себе информацию о температуре каждого из 3-х БУМ в каждый момент времени.\n","\n","Для простоты рассмотрим только 1-й блок и будем прогнозировать только его температуру."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zev7xJwEtL8G"},"outputs":[],"source":["df = df[['mode','temp1']].rename(columns={'temp1':'temp'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DnbNkZpZtL8H"},"outputs":[],"source":["plt.plot(df['temp'])\n","plt.xlim(0,1000)\n","plt.xlabel('Time')\n","plt.ylabel('Temp')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wr0ixJHStL8I"},"source":["## Подготовка данных"]},{"cell_type":"markdown","metadata":{"id":"y2GgmMJCtL8I"},"source":["### Нормирование"]},{"cell_type":"markdown","source":["#### **Задание 1** "],"metadata":{"id":"VitHdSD7tRuH"}},{"cell_type":"markdown","metadata":{"id":"TIbuvRQ_tL8J"},"source":["Температура, записанная в таблице **df** измеряется в градусах Цельсия. Однако для применения различных моделей машинного обучения желательно нормировать данные, чтобы ограничить их максимальные и минимальные значения. Отнормируем значения температуры на отрезок \\[0,1\\]. Сохраненные значения **max_value** и **min_value** позволят после выполнения прогноза вернуться к реальным значениям температуры.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbAXvcEetL8J"},"outputs":[],"source":["def normalize(series):\n","    # Напишите здесь свой код\n","    \n","    return normalized, max_value, min_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtj0d-uctL8K"},"outputs":[],"source":["normalized, max_value, min_value = normalize(df['temp'])\n","df.loc[:,'temp'] = normalized"]},{"cell_type":"markdown","metadata":{"id":"w3zWhos7tL8L"},"source":["### Создание тестовой выборки"]},{"cell_type":"markdown","metadata":{"id":"BLG0oVottL8M"},"source":["В качестве тестовой выборки будем использовать последовательность значений температур определенной длины **seq_length** и режим работы в конце этой последовательности. При этом стоит обратить внимание на то, что режим работы в какой-то временной отсчет $mode_i$ определяет значение температуры в тот же самый отсчет времени $T_i$. Поэтому обучающая выборка должна состоять из значений:\n","\n","$$\n","X = \\{T_{N-seq\\_length+1}, ..., T_{N-1}, T_{N}, mode_{N+1}\\}.\n","$$\n","\n","В то время как целевая переменная является просто последующим значением температуры:\n","\n","$$\n","y = T_{N+1}.\n","$$\n","\n","Воспользуемся функцией, написанной в прошлой лабораторной работе, которая будет создавать обучающую и тестовую выборку объемом **N**, где длина обучающей последовательности равна **seq_length**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85-BOer7tL8N"},"outputs":[],"source":["def get_sequences(df, N, seq_length):\n","    if N + seq_length > len(df) - 1:\n","        N = len(df) - seq_length - 1\n","    X = None\n","    y = None\n","    pos = 1\n","    while pos < N:\n","        seq = np.append(df.loc[pos:pos+seq_length-1, 'temp'].values, df.loc[pos+seq_length, 'mode'].astype('float64'))\n","        X = np.vstack((X, seq)) if X is not None else seq\n","        y_val = df.loc[pos+seq_length, 'temp'].astype('float64')\n","        y = np.append(y, y_val) if y is not None else np.array(y_val)\n","        pos += 1\n","    return X, y"]},{"cell_type":"markdown","metadata":{"id":"av_lLUP4tL8O"},"source":["Создадим обучающую и тестовую выборки"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5ms16SptL8O"},"outputs":[],"source":["size = 10000\n","seq_length = 40\n","\n","X, y = get_sequences(df, size, seq_length)"]},{"cell_type":"markdown","metadata":{"id":"ZGS0A0jctL8P"},"source":["## Ансамблевые модели машинного обучения"]},{"cell_type":"markdown","metadata":{"id":"eqpgacVMtL8P"},"source":["Мотивация применения ансамблевых методов состоит в том, чтобы объединить прогнозы нескольких базовых моделей, построенных с заданным алгоритмом обучения и улучшить качество прогноза.\n","\n","Обычно выделяют два семейства ансамблевых методов:\n","\n","- в методах усреднения (бэггинга) главный принцип состоит в том, чтобы построить несколько моделей независимо, а затем усреднить их прогнозы. В среднем, комбинированная оценка обычно лучше, чем любая из оценок отдельной модели, потому что дисперсия ее ошибки прогноза уменьшается.\n","\n","- в методах бустинга базовые модели строятся последовательно, и каждая пытается уменьшить ошибку прогноза предыдущей модели. Идея состоит в том, чтобы объединить несколько слабых моделей для создания мощного ансамбля. "]},{"cell_type":"markdown","metadata":{"id":"JLdBGFQGtL8Q"},"source":["## Бэггинг"]},{"cell_type":"markdown","metadata":{"id":"cAETmpjHtL8R"},"source":["В ансамблевых алгоритмах бэггинга создается несколько экземпляров базовой модели на случайных подмножествах исходной обучающей выборки, а затем агрегируются индивидуальные прогнозы этих моделей для формирования окончательного прогноза. Такой подход используется как способ уменьшить дисперсию базовой модели (например, дерева решений) путем введения случайности в процедуру построения моделей и последующего создания из них ансамбля. Во многих случаях методы бэггинга представляют собой очень простой способ улучшения базовой модели, без необходимости ее специальной настройки. Поскольку бэггинг обеспечивает способ уменьшения переобучения, этот подход лучше всего работает с сильными и сложными моделями (например, полностью разработанными деревьями решений), в отличие от методов бустинга, которые обычно лучше всего работают со слабыми моделями (например, с неглубокими деревьями решений).\n","\n","Методы бэггинга бывают разных видов, но в основном отличаются друг от друга тем, как они создают случайные подмножества обучающей выборки:\n","\n","- метод вставки - обучающие выборки создаются как случайные подмножества исходной выборки.\n","\n","- метод бутстрэпа - обучающие выборки создаются как исходная выборка с повторениями, т.е. некоторые, случайные элементы исходной выборки дублируются.\n","\n","- метод случайного подпространства - обучающие выборки создаются как случайные подмножества признаков.\n","\n","- метод случайных исправлений - базовые модели обучаются на подмножествах как исходной обучающей выборки, так и признаков."]},{"cell_type":"markdown","metadata":{"id":"psTrdrEltL8S"},"source":["### Создание обучающей выборки методом бутстрэпа"]},{"cell_type":"markdown","source":["#### **Задание 2** "],"metadata":{"id":"6gSVTsEHtbbK"}},{"cell_type":"markdown","metadata":{"id":"tkBh2Ha_tL8S"},"source":["Реализуйте метод формирования обучающих выборок бутстрэпом. Функция **get_bootstrap_samples(data_X, data_y, n_samples)** принимает на вход исходную выборку **data_X, data_y** и число выходных обучающих выборок **n_samples**, а возвращает совокупность обучающих выборок размером **(n_samples, len(data_X))**. Внутри функции необходимо случайным образом из исходной выборки **n_samples** раз выбрать элементы (с повторениями)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiSlO1WatL8T"},"outputs":[],"source":["def get_bootstrap_samples(data_X, data_y, n_samples):\n","    # Напишите здесь свой код\n","    \n","    samples_X = ...\n","    samples_y = ...\n","    return (samples_X, samples_y)"]},{"cell_type":"markdown","source":["#### **Задание 3** "],"metadata":{"id":"HNeChwsTtc05"}},{"cell_type":"markdown","metadata":{"id":"kqJD1PoGtL8U"},"source":["Реализуйте функцию **regr_mape(y_pred, y_test)**, которая подсчитывает значение метрики **MAPE** (Mean Absolut Percentage Error - средняя абсолютная ошибка) для прогнозных **y_pred** и истинных значений **y_test** температуры. Также реализуйте функцию **train(model, model_name, evaluate, X_train, y_train, X_test, y_test)**, которая будет обучать заданную модель **model**. Переменная **evaluate** обозначает метод, который используется для оценки точности прогнозных значений (например, **regr_mape()**). Функция **train()** должна выдавать на выходе значение точности прогноза на обучающей выборке, точности прогноза на тестовой выборке и массив самих прогнозных значений. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_hhptGptL8V"},"outputs":[],"source":["def regr_mape(y_pred, y_test):\n","    # Напишите здесь свой код\n","    \n","    return mape\n","\n","def train(model, model_name, evaluate, X_train, y_train, X_test, y_test):\n","    # Напишите здесь свой код\n","    \n","    return (score_train, score_test, y_pred)"]},{"cell_type":"markdown","metadata":{"id":"Av4ZAoyEtL8W"},"source":["Создайте 1000 обучающих выборок и обучите на них 1000 моделей **DecisionTreeRegressor** с оптимальными параметрами, найденными в предыдущей лабораторной работе. Затем получите прогнозы температуры для тестовой выборки."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0U60-FGtL8W"},"outputs":[],"source":["n_samples = 1000\n","train_len = 8000\n","X_train = X[:train_len]\n","y_train = y[:train_len]\n","X_test = X[train_len:]\n","y_test = y[train_len:]"]},{"cell_type":"markdown","source":["#### **Задание 4** "],"metadata":{"id":"uN7tZrRWtgLR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qByurzSDtL8X"},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","dtr = DecisionTreeRegressor(max_depth=9, min_samples_split=3, min_samples_leaf=2, random_state=0)\n","\n","results = get_bootstrap_samples(X_train, y_train, n_samples)\n","predictions = []\n","\n","# Напишите здесь свой код\n","X_bootstrap = ...\n","y_bootstrap = ...\n","\n","predictions = np.asarray(predictions)"]},{"cell_type":"markdown","metadata":{"id":"6sadf40jtL8Y"},"source":["### Прогнозирование временного ряда методом бутстрэпа"]},{"cell_type":"markdown","metadata":{"id":"RR7MkztxtL8Z"},"source":["Техника бутстрэпа позволяет не просто выдать прогноз в виде определенного значения, но выдать прогноз в виде набора возможных значений, что позволяет оценить доверительный интервал для полученного прогноза. Для примера построим гистограмму распределения для первого прогнозного значения температуры."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DfgR2EItL8Z"},"outputs":[],"source":["plt.figure()\n","plt.hist(predictions[:,0], bins=100)\n","plt.xlabel('Pred temp')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"markdown","source":["#### **Задание 5** "],"metadata":{"id":"mT_JpjNwto1o"}},{"cell_type":"markdown","metadata":{"id":"bC8aCZGAtL8a"},"source":["Напишите функцию **stat_intervals(stat, alpha)** для определения доверительного интервала для набора данных **stat** с заданным доверительным уровнем ошибки **alpha** (т.е. если alpha = 0.1, то вероятность того, что искомая величина попадает в доверительный интервал равна 90 %). Функция должна выдавать пару значений: верхнюю и нижнюю границы."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4T136DDtL8a"},"outputs":[],"source":["def stat_intervals(stat, alpha):\n","    # Напишите здесь свой код\n","    \n","    return boundaries"]},{"cell_type":"markdown","metadata":{"id":"gtK8ak00tL8b"},"source":["Определим доверительный интервал первого спрогнозированного значения для вероятности 95 %."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7T8xi1d5tL8c"},"outputs":[],"source":["interval = stat_intervals(predictions[:,0], 0.05)\n","interval"]},{"cell_type":"markdown","metadata":{"id":"JKpi5IJTtL8c"},"source":["Функция **train()** в результате своей работы также выдала результаты прогноза значений температуры на длительный интервал времени. На основании спрогнозированных значений температуры множеством моделей получите для каждого спрогнозированного значения среднее, а также доверительный интервал с уровнем доверия 95 %. Далее постройте график изменения температуры, на котором отразите истинные значения температуры, средние спрогнозированные значения, а также доверительные интервалы."]},{"cell_type":"markdown","source":["#### **Задание 6** "],"metadata":{"id":"JbvLdlUStrTc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7QX1tpdgtL8d"},"outputs":[],"source":["alpha = 0.05\n","\n","# Напишите здесь свой код\n","means = ...\n","boundaries = ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFQHTdfOtL8e"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","length = 50\n","x1 = np.linspace(start=0, stop=length, num=length, endpoint=False)\n","x2 = np.linspace(start=length, stop=2*length, num=length, endpoint=False)\n","plt.plot(x1, y_train[-length:], c='black')\n","plt.plot(x2, y_test[:length], c='black', label='true')\n","plt.legend()\n","plt.xlabel('timestamp')\n","plt.ylabel('normalized temp')\n","plt.title('DecisionTreeRegressor + bootstrap')\n","\n","# Напишите здесь свой код\n","\n","plt.show()"]},{"cell_type":"markdown","source":["#### **Задание 7** "],"metadata":{"id":"Zi3lV-sZtzID"}},{"cell_type":"markdown","metadata":{"id":"FO16BrpOtL8e"},"source":["Определите точность усредненного прогноза и сравните его с точностью прогноза одной модели **DecisionTreeRegressor**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAKpNVmPtL8f"},"outputs":[],"source":["# Напишите здесь свой код\n","score_bootstrap_dtr = ...\n","\n","dtr = DecisionTreeRegressor(max_depth=9, min_samples_split=3, min_samples_leaf=2, random_state=0)\n","results = train(dtr, 'dtr', regr_mape, X_train, y_train, X_test, y_test)\n","score_dtr = results[1]\n","\n","print('MAPE for single model: {:.5f}'.format(score_dtr))\n","print('MAPE for bootstrap model: {:.5f}'.format(score_bootstrap_dtr))"]},{"cell_type":"markdown","metadata":{"id":"1UUshvPMtL8g"},"source":["**Выводы:** \n","\n","сделайте выводы"]},{"cell_type":"markdown","metadata":{"id":"P8hmCo1htL8g"},"source":["### Случайный лес"]},{"cell_type":"markdown","metadata":{"id":"e0MFSjgutL8h"},"source":["Другой реализацией бэггинга (метода устреднения) является модель под названием \"случайный лес\", или **RandomForest**. Он может применяться как для решения задачи классификации - [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier \"RandomForestClassifier\"), так и для решения задачи регрессии - [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor \"RandomForestRegressor\"). Назван эта модель так, потому что она состоит из нескольких моделей **DecisionTree**, у каждой из которых случайным образом выбраны два параметры: наполнение обучающей выборки и максимальное число используемых признаков. По сути это и есть реализация метода бутстрэп и метода случайного подпространства признаков (о которых речь была выше), но на моделях решающего дерева. Итоговый прогноз получается в результате усреднения прогноза каждой из моделей. За счет рандомизации параметров моделей и их большого числа достигается увеличение качества работы \"случайного леса\".\n","\n","Обучите модель **RandomForestRegressor** на тех же данных, что и нашу собственную модель на основе бутсnрэпа. Параметры у модели **RandomForestRegressor** такие же, как и у **DecisionTreeRegressor**, только добавляется **n_estimators** - число простых моделей в итоговой модели, **max_features** - максимальное число признаков, которое будет анализироваться, и **bootstrap** - использовать или нет метод бутстрэпа. Подберите оптимальное значение **n_estimators**, **max_depth**, **min_samples_split**."]},{"cell_type":"markdown","source":["#### **Задание 8** "],"metadata":{"id":"6e4oSb86t7p7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1f-NjDU9tL8i"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","rf = RandomForestRegressor(\n","    n_estimators = 10, \n","    max_depth = 2, \n","    min_samples_split = 2, \n","    random_state = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuDZQTmPtL8j"},"outputs":[],"source":["# Напишите здесь свой код\n"]},{"cell_type":"markdown","source":["#### **Задание 9** "],"metadata":{"id":"Y18QSIF3t9ur"}},{"cell_type":"markdown","metadata":{"id":"g5aNM6tZtL8j"},"source":["Обучите модель с оптимальными параметрами, найденными выше. Определите финальное качество прогноза на тестовой выборке. Постройте график прогнозных значений температуры."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxxBamgCtL8k"},"outputs":[],"source":["# Напишите здесь свой код\n","rf = RandomForestRegressor(\n","    n_estimators = ..., \n","    max_depth = ..., \n","    min_samples_split = ..., \n","    random_state = 0)\n","(score_train, score_rf, preds) = train(rf, 'rf', regr_mape, X_train, y_train, X_test, y_test)\n","\n","print('MAPE for RandomForest model: {:.5f}'.format(score_rf))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcglaopKtL8l"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","length = 50\n","x1 = np.linspace(start=0, stop=length, num=length, endpoint=False)\n","x2 = np.linspace(start=length, stop=2*length, num=length, endpoint=False)\n","plt.plot(x1, y_train[-length:], c='black')\n","plt.plot(x2, y_test[:length], c='black', label='true')\n","plt.legend()\n","plt.xlabel('timestamp')\n","plt.ylabel('normalized temp')\n","plt.title('RandomForestRegressor')\n","\n","# Напишите здесь свой код\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8XzkEnTEtL8m"},"source":["## Метод бустинга"]},{"cell_type":"markdown","metadata":{"id":"HwOBzmritL8m"},"source":["### AdaBoost"]},{"cell_type":"markdown","metadata":{"id":"JUtuzIGStL8n"},"source":["Эта модель является одной из самых популярных реализаций методов бустинга и была разработана в 1995 году Фройндом и Шапиро. Основной принцип AdaBoost состоит в том, чтобы обучить последовательность \"слабых\" моделей (то есть моделей, которые лишь немного лучше, чем случайный прогноз, такие как небольшие деревья решений) на многократно изменяемых версиях данных. Прогнозы от всех из них затем объединяются посредством взвешенного большинства голосов (или суммы) для получения окончательного прогноза. Модификации данных на каждой так называемой итерации бустинга состоят в применении весов к каждой обучающей выборке. Первоначально все эти веса установлены равными, поэтому на первом этапе просто обучается \"слабая\" модель на исходных данных. Для каждой последующей итерации веса выборки индивидуально изменяются, и алгоритм обучения повторно применяется к повторно взвешенным данным. На данном этапе те обучающие примеры, которые были неправильно предсказаны усиленной моделью, созданной на предыдущем шаге, имеют увеличенные веса, тогда как веса уменьшаются для тех, которые были предсказаны правильно. По мере продолжения итераций, примеры, которые трудно предсказать, получают все большее влияние. Таким образом, каждая последующая \"слабая\" модель вынуждена концентрироваться на примерах, которые упускают предыдущие модели. "]},{"cell_type":"markdown","metadata":{"id":"XVn8Hoy8tL8o"},"source":["Модель AdaBoost позволяет решать как задачи классификации, так и задачи регрессии. Создайте объект [**AdaBoostRegressor**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor \"AdaBoostRegressor\"). Параметр **base_estimator** задает \"слабую\" модель, которая будет использоваться для бустинга, **n_estimators** обозначает число \"слабых\" моделей в последовательности. Параметр **learning_rate** определяет вес, на который домножаются примеры из обучающей выборки. Этот параметр очень важен и играет роль регуляризации: чем он меньше, тем медленнее идет обучение, но тем \"лучше\" оно проходит. Наконец параметр **loss** определяет функцию (она называется функция потерь), по которой на каждой итерации бустинга рассчитывается штраф за неверно предсказанное значение и определяются веса, на которые нужно домножить примеры из обучающей выборки на следующей итерации бустинга. Доступные функции потерь - линейная, квадратичная и экспоненциальная.\n","\n","Обучите модель AdaBoost на двух \"слабых\" моделях: решающее дерево (с оптимальными параметрами, найденными выше) и линейная регрессия. Подберите оптимальным образом параметры **learning_rate** и **n_estimators** Определите качество прогноза на тех же самых тестовых данных, что и выше."]},{"cell_type":"markdown","source":["#### **Задание 10** "],"metadata":{"id":"THozXHvDuIuB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8EJVjcWtL8o"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.linear_model import LinearRegression\n","\n","dtr = DecisionTreeRegressor(max_depth=10, min_samples_split=2, random_state=0)\n","lr = LinearRegression()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkelTPDDtL8p"},"outputs":[],"source":["# Напишите здесь свой код\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKnwWodctL8q"},"outputs":[],"source":["# Напишите здесь свой код\n","adaboost_dtr = AdaBoostRegressor(\n","        base_estimator = dtr,\n","        n_estimators = ..., \n","        learning_rate = ..., \n","        random_state = 0)\n","adaboost_lr = AdaBoostRegressor(\n","        base_estimator = lr,\n","        n_estimators = ..., \n","        learning_rate = ..., \n","        random_state = 0)\n","(score_train, score_adaboost_dtr, preds_adaboost_dtr) = train(adaboost_dtr, 'adaboost_dtr', regr_mape, X_train, y_train, X_test, y_test)\n","(score_train, score_adaboost_lr, preds_adaboost_lr) = train(adaboost_lr, 'adaboost_lr', regr_mape, X_train, y_train, X_test, y_test)\n","\n","print('MAPE for AdaBoost on DecisionTree model: {:.5f}'.format(score_adaboost_dtr))\n","print('MAPE for AdaBoost on LinearRegression model: {:.5f}'.format(score_adaboost_lr))"]},{"cell_type":"markdown","source":["#### **Задание 11** "],"metadata":{"id":"Ky2bjNQ7uSOa"}},{"cell_type":"markdown","metadata":{"id":"RdeoLHGAtL8r"},"source":["Постройте сравнительный график прогнозных значений температуры обоих моделей."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbVUkJsitL8r"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","length = 50\n","x1 = np.linspace(start=0, stop=length, num=length, endpoint=False)\n","x2 = np.linspace(start=length, stop=2*length, num=length, endpoint=False)\n","plt.plot(x1, y_train[-length:], c='black')\n","plt.plot(x2, y_test[:length], c='black', label='true')\n","plt.legend()\n","plt.xlabel('timestamp')\n","plt.ylabel('normalized temp')\n","plt.title('AdaBoost')\n","\n","# Напишите здесь свой код\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_xFKcP2gtL8s"},"source":["**Выводы:**\n","\n","сделайте выводы"]},{"cell_type":"markdown","metadata":{"id":"WQ3Y9gS0tL8s"},"source":["### Gradient Tree Boosting"]},{"cell_type":"markdown","metadata":{"id":"mDjhVvTctL8t"},"source":["В модели **AdaBoost** есть возможность выбора функции потерь, которая определяет каким образом будут распределены веса между примерами из обучающей выборки на следующей итерации бустинга, - линеная, квадратичная или экспоненциальная. Градиентный бустинг на решающих деревьях (Gradient Tree Boosting или **Gradient Boosted Decision Tree, GBDT**) - это обобщение на случай произвольных дифференцируемых функций потерь. **GBDT** - это эффективная модель процедура, которую можно использовать для решения задач регрессии и классификации в различных областях, включая ранжирование в веб-поиске и экологию.\n","\n","Идея **GBDT** состоит в следующем. Итоговая модель представляет собой линеную комбинацию \"слабых\" моделей - решающих деревьев:\n","\n","$$\n","F_M(x) = \\sum^M_{m=1}h_m(x)\n","$$\n","\n","На каждой итерации бустинга к итоговой модели добавляется новое решающее дерево:\n","\n","$$\n","F_m(x) = F_{m-1}(x) + h_m(x),\n","$$\n","\n","которое выбирается таким образом, чтобы минимизировать ошибку прогноза, т.е. функцию потерь $L(y, F(x))$, уже построенной к этой итерации модели:\n","\n","$$\n","h_m = argmin_{h}L_m = argmin_h\\sum^N_{i=1}L(y_i, F_{m-1}(x_i) + h(x_i))\n","$$\n","\n","Построение этого нового решающего дерева ведется методом градиентного спуска для минимизации функции потерь:\n","\n","$$\n","h_m \\approx argmin_h\\sum^N_{i=1}h(x_i)g_i, \\quad g_i = \\left.\\dfrac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)}\\right|_{F=F_{m-1}}.\n","$$"]},{"cell_type":"markdown","source":["#### **Задание 12** "],"metadata":{"id":"lvG505dquYws"}},{"cell_type":"markdown","metadata":{"id":"AaiNIst7tL8u"},"source":["Постройте и обучите модель градиентного бустинга из пакета **LightGBM** - [LGBMRegressor](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor \"LGbMRegressor\"). Эта реализации работает сильно быстрее по сравнению с реализацией в пакете Sklearn [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor \"GradientBoostingRegressor\"). Параметры этой модели те же самые, что и у **AdaBoost**.\n","\n","Подберите оптимальным образом параметры num_leaves, max_depth, learning_rate и n_estimators. Оцените качетсво работы модели на том же тестовом датасете, что и предыдующие модели."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1gb7GZUtL8u"},"outputs":[],"source":["!pip install lightgbm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMriPpH9tL8v"},"outputs":[],"source":["from lightgbm import LGBMRegressor\n","\n","# Напишите здесь свой код\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6pONnqMtL8v"},"outputs":[],"source":["# Напишите здесь свой код\n","lgbm = LGBMRegressor(num_leaves = ..., \n","                     max_depth = ..., \n","                     learning_rate = ..., \n","                     n_estimators = ..., \n","                     random_state = 0)\n","(score_train, score_lgbm, preds_lgbm) = train(lgbm, 'lgbm', regr_mape, X_train, y_train, X_test, y_test)\n","\n","print('MAPE for LightGBM: {:.5f}'.format(score_lgbm))"]},{"cell_type":"markdown","source":["#### **Задание 13** "],"metadata":{"id":"-UXlk7WjuaoG"}},{"cell_type":"markdown","metadata":{"id":"5_gpXP3gtL8w"},"source":["Постройте график прогнозных значений температуры."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_56TR90ztL8w"},"outputs":[],"source":["plt.figure(figsize=(5,5))\n","length = 50\n","x1 = np.linspace(start=0, stop=length, num=length, endpoint=False)\n","x2 = np.linspace(start=length, stop=2*length, num=length, endpoint=False)\n","plt.plot(x1, y_train[-length:], c='black')\n","plt.plot(x2, y_test[:length], c='black', label='true')\n","plt.legend()\n","plt.xlabel('timestamp')\n","plt.ylabel('normalized temp')\n","plt.title('LightGBM')\n","\n","# Напишите здесь свой код\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"IUCaXNz2tL8w"},"source":["## Сравнение различных методов и моделей"]},{"cell_type":"markdown","metadata":{"id":"636NgWi2tL8x"},"source":["Постройте для сравнения на одной гистограмме качество проанализированных моделей на тестовой выборке: обычное решающее дерево, решающее дерево с бутстрэпом, случайный лес, AdaBoost на линейной регрессии, AdaBoost на решающем дереве, градиентный бустинг на решающих деревьях LightGBM. Сделайте выводы."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fDQNjj5tL8x"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","xx = ['DT', 'DT with Bootstrap', 'Random Forest', 'AdaBoost on LR', 'AdaBoost on DT', 'LightGBM']\n","yy = [score_dtr, score_bootstrap_dtr, score_rf, score_adaboost_lr, score_adaboost_dtr, score_lgbm]\n","rects = plt.bar(xx, yy)\n","plt.ylim(0, 0.025)\n","plt.ylabel('MAPE')\n","for i, rect in enumerate(rects):\n","    yloc = rect.get_height()\n","    xloc = rect.get_x() + rect.get_width() / 4\n","    plt.annotate(round(yy[i], 4), xy=(xloc, yloc), xytext=(xloc, 10),\n","                            textcoords=\"offset points\",\n","                            va='center',\n","                            color='black', clip_on=True)"]},{"cell_type":"markdown","source":["#### **Задание 14** "],"metadata":{"id":"F8k57-f8ueEz"}},{"cell_type":"markdown","metadata":{"id":"8hvAs9PPtL8y"},"source":["**Выводы:**\n","\n","сделайте выводы"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2T1gU1XtL8y"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}